{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # This is Notebook for File Type detection using convolutional neural network on TREC-DD dataset\n",
    " # The data used is in 256 byte frequencies format\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from utility import *\n",
    "from sklearn.metrics import classification_report\n",
    "fileshape = 0\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  \n",
    "  config = params\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  \n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 256, 1, 1])\n",
    "  print(input_layer.shape)\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 1]\n",
    "  # Output Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # kernel_size specifies [width, height]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[config['kernel1_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv1.shape)\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # pool_size, strides [width, height]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[config['pool1_width'], 1], strides=[config['pool1_stride'],1])\n",
    "  print(pool1.shape)\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x1 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[config['kernel2_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv2.shape)\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[config['pool2_width'], 1], strides=[config['pool2_stride'],1])\n",
    "  print(pool2.shape)\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64 * 1* 64]]\n",
    "  pool2_shape = pool2.shape\n",
    "  pool2_flat = tf.reshape(pool2, [-1, pool2_shape[1] * pool2_shape[2] * pool2_shape[3]])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 64 * 1* 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=config['dense_units'], activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 93]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=config['nclasses'])\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=config['nclasses'])\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unused_argv = ['pilot_1d_cnn_newAPI.py', 'temp_data.csv', 'model/', 1024, 512, 256]\n",
    "\n",
    "filename = unused_argv[1]\n",
    "\n",
    "if os.path.isfile('ft_to_idx.npy') and os.path.isfile('nclasses.npy') and os.path.isfile('group_data.npy'):\n",
    "    ft_to_idx = np.load('ft_to_idx.npy')\n",
    "    ft_to_idx = ft_to_idx.item()\n",
    "    nclasses = np.load('nclasses.npy')\n",
    "    #f = open('group_data.pkl','r')\n",
    "    #group_data = pickle.load('group_data.pkl')\n",
    "    #group_data = np.load('group_data.npy') \n",
    "    group_data = np.load('toy_data.npy') # for proof of algo purpose, real use case should use the above line\n",
    "    group_data = group_data.item()\n",
    "\n",
    "else:\n",
    "    ft_to_idx, nclasses, group_data = prepare_file(filename)\n",
    "    np.save(\"ft_to_idx\", ft_to_idx)\n",
    "    np.save(\"nclasses\", nclasses)\n",
    "    #f = open('group_data.pkl','w')\n",
    "    #pickle.dump(group_data, f)\n",
    "    np.save(\"group_data\", group_data)\n",
    "\n",
    "#gen train sets\n",
    "train_full, test = train_dev_split(group_data, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train, dev = train_dev_split(train_full, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train_full_data, train_full_labels = gen_feed(train_full, ft_to_idx, upper_limit=5000)\n",
    "train_data, train_labels = gen_feed(train, ft_to_idx, upper_limit=5000)\n",
    "dev_data, dev_labels = gen_feed(dev, ft_to_idx, upper_limit=5000)\n",
    "test_data, test_labels = gen_feed(test, ft_to_idx, upper_limit=5000)\n",
    "train_full_data = train_full_data.astype(np.float32)\n",
    "train_data = train_data.astype(np.float32) \n",
    "dev_data = dev_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32) \n",
    "\n",
    "# set config\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "config['model_dir'] = unused_argv[2]\n",
    "config['n_hidden1'] = int(unused_argv[3])\n",
    "config['n_hidden2'] = int(unused_argv[4])\n",
    "config['n_hidden3'] = int(unused_argv[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpo7imtm\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11c492210>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpo7imtm', '_save_summary_steps': 100}\n",
      "(16, 256, 1, 1)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 64)\n",
      "(16, 256, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpo7imtm/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00827962  0.00816503  0.0082639  ...,  0.00810003  0.00813184\n",
      "   0.00818341]\n",
      " [ 0.00810141  0.00792431  0.00849165 ...,  0.0081029   0.00834672\n",
      "   0.00826154]\n",
      " [ 0.00829337  0.00818027  0.00813966 ...,  0.0080619   0.00803615\n",
      "   0.00804139]\n",
      " ..., \n",
      " [ 0.00824315  0.00760389  0.0082547  ...,  0.00775374  0.00792276\n",
      "   0.00821641]\n",
      " [ 0.00913472  0.00796779  0.00794275 ...,  0.00689905  0.00737945\n",
      "   0.00767167]\n",
      " [ 0.00829717  0.0082497   0.00838664 ...,  0.00790112  0.00838704\n",
      "   0.00835948]]\n",
      "INFO:tensorflow:loss = 4.80021, step = 1\n",
      "INFO:tensorflow:global_step/sec: 6.79957\n",
      "INFO:tensorflow:loss = 4.80367, step = 101 (14.717 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpo7imtm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.77647.\n",
      "(?, 256, 1, 1)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 64)\n",
      "(?, 256, 1, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-18-22:47:47\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpo7imtm/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-18-22:47:48\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0119048, global_step = 200, loss = 4.792\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.4\n",
      "dense units: 1024\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 1\n",
      "pool1 pool2 stride: 1\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7920008, 'global_step': 200, 'accuracy': 0.011904762}\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp7swtpp\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11b221650>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp7swtpp', '_save_summary_steps': 100}\n",
      "(16, 256, 1, 1)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 64)\n",
      "(16, 256, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp7swtpp/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00820301  0.0079263   0.00849973 ...,  0.00814475  0.00827076\n",
      "   0.00816656]\n",
      " [ 0.00834329  0.00814885  0.0082064  ...,  0.00821667  0.00816471\n",
      "   0.00844167]\n",
      " [ 0.00796863  0.00828371  0.00817441 ...,  0.00817367  0.00811881\n",
      "   0.00805024]\n",
      " ..., \n",
      " [ 0.0081913   0.00800729  0.00833743 ...,  0.00820394  0.00831417\n",
      "   0.00822363]\n",
      " [ 0.008176    0.00823348  0.0081687  ...,  0.00828924  0.00823786\n",
      "   0.00839483]\n",
      " [ 0.0080115   0.00809342  0.00842063 ...,  0.0080146   0.0082791\n",
      "   0.00850334]]\n",
      "INFO:tensorflow:loss = 4.81573, step = 1\n",
      "INFO:tensorflow:global_step/sec: 14.1074\n",
      "INFO:tensorflow:loss = 4.81296, step = 101 (7.085 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp7swtpp/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.80426.\n",
      "(?, 256, 1, 1)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 64)\n",
      "(?, 256, 1, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-18-22:48:10\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp7swtpp/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-18-22:48:11\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0297619, global_step = 200, loss = 4.79451\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.4\n",
      "dense units: 512\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 1\n",
      "pool1 pool2 stride: 1\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7945089, 'global_step': 200, 'accuracy': 0.029761905}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "dense_units = [1024, 512]#, 256]\n",
    "batch_sizes = [16]#,32,64,256,512]\n",
    "dropouts = [0.4]#,0.2]\n",
    "kernel_width = [3]#,5,7]\n",
    "pool_width = [1]#,2,4]\n",
    "pool_stride = [1]#,2,4]\n",
    "\n",
    "\n",
    "#reset config, defined in previous block\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "results = []\n",
    "for dense_unit,batch_size,dropout,kernel_width,pool_width,pool_stride in product(dense_units,batch_sizes,dropouts,kernel_width,pool_width,pool_stride):\n",
    "    config['dropout'] = dropout\n",
    "    config['dense_units'] = dense_unit\n",
    "    config['kernel1_width'] = kernel_width\n",
    "    config['kernel2_width'] = kernel_width\n",
    "    config['pool1_width'] = pool_width\n",
    "    config['pool2_width'] = pool_width\n",
    "    config['pool1_stride'] = pool_stride\n",
    "    config['pool2_stride'] = pool_stride\n",
    "    \n",
    "    # Create the Estimator\n",
    "    cnn_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=None, params=config)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "    cnn_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=200, # 60000\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": dev_data},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    eval_results = cnn_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print ('\\n----setting----')\n",
    "    print ('batch_size:', batch_size)\n",
    "    print ('dropout:', dropout)\n",
    "    print ('dense units:', dense_unit)\n",
    "    print ('kernel1 kernel2 width:', kernel_width)\n",
    "    print ('pool1 pool2 width:', pool_width)\n",
    "    print ('pool1 pool2 stride:', pool_stride)\n",
    "    print ('----performance----\\n')\n",
    "    print(eval_results)\n",
    "    results.append((eval_results['accuracy'], batch_size, dropout, dense_unit, kernel_width, pool_width, pool_stride))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = sorted(results, key=lambda x: x[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11b0b8b90>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'model/', '_save_summary_steps': 100}\n",
      "(16, 256, 1, 1)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 32)\n",
      "(16, 256, 1, 64)\n",
      "(16, 256, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into model/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00805537  0.00827004  0.00792685 ...,  0.00885185  0.00843289\n",
      "   0.00815173]\n",
      " [ 0.00683125  0.00751098  0.00736588 ...,  0.00894429  0.00780404\n",
      "   0.00860996]\n",
      " [ 0.00821552  0.0079246   0.00808118 ...,  0.00825887  0.00811211\n",
      "   0.00870418]\n",
      " ..., \n",
      " [ 0.00799329  0.00823984  0.00819944 ...,  0.00859531  0.00818598\n",
      "   0.00820814]\n",
      " [ 0.00810589  0.00809389  0.00826326 ...,  0.00841312  0.00796292\n",
      "   0.00839545]\n",
      " [ 0.00804805  0.0078403   0.00778795 ...,  0.00901834  0.00814685\n",
      "   0.00812148]]\n",
      "INFO:tensorflow:loss = 4.78603, step = 1\n",
      "INFO:tensorflow:global_step/sec: 14.0191\n",
      "INFO:tensorflow:loss = 4.80061, step = 101 (7.131 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.80727.\n",
      "(?, 256, 1, 1)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 32)\n",
      "(?, 256, 1, 64)\n",
      "(?, 256, 1, 64)\n",
      "INFO:tensorflow:Restoring parameters from model/model.ckpt-200\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         2\n",
      "          1       0.00      0.00      0.00         2\n",
      "          2       0.00      0.00      0.00         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         2\n",
      "          5       0.00      0.00      0.00         2\n",
      "          6       0.00      0.00      0.00         2\n",
      "          7       0.00      0.00      0.00         2\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       0.00      0.00      0.00         2\n",
      "         11       0.00      0.00      0.00         2\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00         2\n",
      "         14       0.00      0.00      0.00         2\n",
      "         15       0.00      0.00      0.00         2\n",
      "         16       0.00      0.00      0.00         2\n",
      "         17       0.00      0.00      0.00         2\n",
      "         18       0.00      0.00      0.00         2\n",
      "         19       0.00      0.00      0.00         2\n",
      "         20       0.00      0.00      0.00         2\n",
      "         21       0.00      0.00      0.00         2\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       0.00      0.00      0.00         2\n",
      "         24       0.00      0.00      0.00         2\n",
      "         25       0.00      0.00      0.00         2\n",
      "         26       0.00      0.00      0.00         2\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       0.00      0.00      0.00         2\n",
      "         29       0.00      0.00      0.00         2\n",
      "         30       0.00      0.00      0.00         2\n",
      "         31       0.00      0.00      0.00         2\n",
      "         33       0.00      0.00      0.00         2\n",
      "         34       0.00      0.00      0.00         2\n",
      "         35       0.00      0.00      0.00         2\n",
      "         36       0.00      0.00      0.00         1\n",
      "         37       0.00      0.00      0.00         2\n",
      "         38       0.00      0.00      0.00         1\n",
      "         39       0.00      0.00      0.00         2\n",
      "         40       0.00      0.00      0.00         2\n",
      "         41       0.00      0.00      0.00         2\n",
      "         42       0.00      0.00      0.00         2\n",
      "         43       0.00      0.00      0.00         1\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       0.00      0.00      0.00         2\n",
      "         46       0.00      0.00      0.00         1\n",
      "         47       0.00      0.00      0.00         2\n",
      "         48       0.00      0.00      0.00         2\n",
      "         50       0.00      0.00      0.00         2\n",
      "         52       0.00      0.00      0.00         2\n",
      "         53       0.00      0.00      0.00         2\n",
      "         54       0.00      0.00      0.00         2\n",
      "         55       0.00      0.00      0.00         2\n",
      "         56       0.00      0.00      0.00         1\n",
      "         57       0.00      0.00      0.00         2\n",
      "         58       0.00      0.00      0.00         1\n",
      "         61       0.00      0.00      0.00         2\n",
      "         62       0.00      0.00      0.00         2\n",
      "         63       0.33      1.00      0.50         2\n",
      "         64       0.00      0.00      0.00         2\n",
      "         65       0.02      0.50      0.04         2\n",
      "         66       0.00      0.00      0.00         1\n",
      "         67       0.00      0.00      0.00         2\n",
      "         68       0.00      0.00      0.00         2\n",
      "         69       0.00      0.00      0.00         2\n",
      "         70       0.00      0.00      0.00         1\n",
      "         71       0.00      0.00      0.00         2\n",
      "         72       0.00      0.00      0.00         2\n",
      "         73       0.00      0.00      0.00         2\n",
      "         74       0.00      0.00      0.00         2\n",
      "         75       0.00      0.00      0.00         2\n",
      "         76       0.00      0.00      0.00         2\n",
      "         78       0.00      0.00      0.00         2\n",
      "         79       0.00      0.00      0.00         2\n",
      "         80       0.00      0.00      0.00         2\n",
      "         82       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         2\n",
      "         90       0.00      0.00      0.00         2\n",
      "         92       0.00      0.00      0.00         2\n",
      "         93       0.00      0.00      0.00         2\n",
      "         94       0.00      0.00      0.00         2\n",
      "         95       0.00      0.00      0.00         2\n",
      "         96       0.00      0.00      0.00         2\n",
      "         97       0.00      0.00      0.00         1\n",
      "         98       0.00      0.00      0.00         2\n",
      "        100       0.00      0.00      0.00         1\n",
      "        103       0.00      0.00      0.00         2\n",
      "        104       0.00      0.00      0.00         1\n",
      "        105       0.00      0.00      0.00         1\n",
      "        106       0.00      0.00      0.00         1\n",
      "        107       0.00      0.00      0.00         1\n",
      "        109       0.00      0.00      0.00         1\n",
      "        110       0.00      0.00      0.00         1\n",
      "        111       0.00      0.00      0.00         0\n",
      "        112       0.00      0.00      0.00         1\n",
      "        113       0.04      1.00      0.07         2\n",
      "        115       0.00      0.00      0.00         2\n",
      "        116       0.00      0.00      0.00         1\n",
      "        117       0.00      0.00      0.00         1\n",
      "        119       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.00      0.03      0.01       175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "best_param = results[0]\n",
    "batch_size = best_param[1]\n",
    "config['dropout'] = best_param[2]\n",
    "config['dense_units'] = best_param[3]\n",
    "config['kernel1_width'] = best_param[4]\n",
    "config['kernel2_width'] = best_param[4]\n",
    "config['pool1_width'] = best_param[5]\n",
    "config['pool2_width'] = best_param[5]\n",
    "config['pool1_stride'] = best_param[6]\n",
    "config['pool2_stride'] = best_param[6]\n",
    "\n",
    "# Create the Estimator\n",
    "cnn_classifier = tf.estimator.Estimator(\n",
    "model_fn=cnn_model_fn, model_dir='model/', params=config)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": train_full_data},\n",
    "  y=train_full_labels,\n",
    "  batch_size=batch_size,\n",
    "  num_epochs=None,\n",
    "  shuffle=True)\n",
    "\n",
    "cnn_classifier.train(\n",
    "  input_fn=train_input_fn,\n",
    "  steps=200, # 60000\n",
    "  hooks=[logging_hook])\n",
    "\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "predictions = cnn_classifier.predict(input_fn=pred_input_fn)\n",
    "predictions = list(p[\"classes\"] for p in predictions)\n",
    "print (classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
