{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # This is Notebook for File Type detection using convolutional neural network on TREC-DD dataset\n",
    " # The data used is in 3 part of 256 byte frequencies format, first 256, middle, last 256\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from utility import *\n",
    "from sklearn.metrics import classification_report\n",
    "fileshape = 0\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  \n",
    "  config = params\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  \n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 256*3, 1, 1])\n",
    "  print(input_layer.shape)\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 1]\n",
    "  # Output Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # kernel_size specifies [width, height]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[config['kernel1_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv1.shape)\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # pool_size, strides [width, height]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[config['pool1_width'], 1], strides=[config['pool1_stride'],1])\n",
    "  print(pool1.shape)\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x1 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[config['kernel2_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv2.shape)\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[config['pool2_width'], 1], strides=[config['pool2_stride'],1])\n",
    "  print(pool2.shape)\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64 * 1* 64]]\n",
    "  pool2_shape = pool2.shape\n",
    "  pool2_flat = tf.reshape(pool2, [-1, pool2_shape[1] * pool2_shape[2] * pool2_shape[3]])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 64 * 1* 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=config['dense_units'], activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 93]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=config['nclasses'])\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=config['nclasses'])\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unused_argv = ['pilot_1d_cnn_newAPI.py', 'temp_data.csv', 'model/', 1024, 512, 256]\n",
    "filename = unused_argv[1]\n",
    "\n",
    "if os.path.isfile('ft_to_idx.npy') and os.path.isfile('nclasses.npy') and os.path.isfile('group_data.npy'):\n",
    "    ft_to_idx = np.load('ft_to_idx.npy')\n",
    "    ft_to_idx = ft_to_idx.item()\n",
    "    nclasses = np.load('nclasses.npy')\n",
    "    #f = open('group_data.pkl','r')\n",
    "    #group_data = pickle.load('group_data.pkl')\n",
    "    #group_data = np.load('group_data.npy') \n",
    "    group_data = np.load('toy_data.npy') # for proof of algo purpose, real use case should use the above line\n",
    "    group_data = group_data.item()\n",
    "\n",
    "else:\n",
    "    ft_to_idx, nclasses, group_data = prepare_file(filename)\n",
    "    np.save(\"ft_to_idx\", ft_to_idx)\n",
    "    np.save(\"nclasses\", nclasses)\n",
    "    #f = open('group_data.pkl','w')\n",
    "    #pickle.dump(group_data, f)\n",
    "    np.save(\"group_data\", group_data)\n",
    "\n",
    "#gen train set\n",
    "train_full, test = train_dev_split(group_data, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train, dev = train_dev_split(train_full, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train_full_data, train_full_labels = gen_feed(train_full, ft_to_idx, upper_limit=5000)\n",
    "train_data, train_labels = gen_feed(train, ft_to_idx, upper_limit=5000)\n",
    "dev_data, dev_labels = gen_feed(dev, ft_to_idx, upper_limit=5000)\n",
    "test_data, test_labels = gen_feed(test, ft_to_idx, upper_limit=5000)\n",
    "train_full_data = train_full_data.astype(np.float32)\n",
    "train_data = train_data.astype(np.float32) \n",
    "dev_data = dev_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32) \n",
    "\n",
    "# set config\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "config['model_dir'] = unused_argv[2]\n",
    "config['n_hidden1'] = int(unused_argv[3])\n",
    "config['n_hidden2'] = int(unused_argv[4])\n",
    "config['n_hidden3'] = int(unused_argv[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpDTxoRO\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1192a7350>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpDTxoRO', '_save_summary_steps': 100}\n",
      "(16, 768, 1, 1)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 64)\n",
      "(16, 768, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpDTxoRO/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00813622  0.00812796  0.00821665 ...,  0.00816913  0.00820674\n",
      "   0.00843206]\n",
      " [ 0.00840087  0.00833263  0.00824761 ...,  0.00843975  0.00815155\n",
      "   0.00823266]\n",
      " [ 0.00741025  0.00853316  0.00871921 ...,  0.0086459   0.00786648\n",
      "   0.00841622]\n",
      " ..., \n",
      " [ 0.00833176  0.00883653  0.00842736 ...,  0.00842685  0.0077648\n",
      "   0.00777488]\n",
      " [ 0.00827998  0.00804049  0.0083692  ...,  0.00845325  0.00821779\n",
      "   0.00836465]\n",
      " [ 0.00822698  0.00794818  0.00827339 ...,  0.00833053  0.00850396\n",
      "   0.00791691]]\n",
      "INFO:tensorflow:loss = 4.79797, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.63251\n",
      "INFO:tensorflow:loss = 4.81147, step = 101 (37.985 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpDTxoRO/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.79503.\n",
      "(?, 768, 1, 1)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 64)\n",
      "(?, 768, 1, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-18-23:32:37\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpDTxoRO/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-18-23:32:39\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0116279, global_step = 200, loss = 4.74467\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.4\n",
      "dense units: 1024\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 1\n",
      "pool1 pool2 stride: 1\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7446718, 'global_step': 200, 'accuracy': 0.011627907}\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpj7CjqX\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11c484190>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpj7CjqX', '_save_summary_steps': 100}\n",
      "(16, 768, 1, 1)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 64)\n",
      "(16, 768, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpj7CjqX/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00854145  0.00738535  0.00924817 ...,  0.00758413  0.00824738\n",
      "   0.00833566]\n",
      " [ 0.00834569  0.00796816  0.00800876 ...,  0.00797359  0.00929985\n",
      "   0.00801083]\n",
      " [ 0.00884009  0.00812157  0.00865172 ...,  0.00816063  0.00856007\n",
      "   0.00796057]\n",
      " ..., \n",
      " [ 0.00844571  0.00793358  0.00781121 ...,  0.00855126  0.00830567\n",
      "   0.00833476]\n",
      " [ 0.0081357   0.00836292  0.00818516 ...,  0.0082509   0.00822536\n",
      "   0.00788555]\n",
      " [ 0.00845503  0.0088846   0.00893115 ...,  0.0080882   0.00723472\n",
      "   0.00754147]]\n",
      "INFO:tensorflow:loss = 4.78741, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.71691\n",
      "INFO:tensorflow:loss = 4.75659, step = 101 (36.803 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpj7CjqX/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.78615.\n",
      "(?, 768, 1, 1)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 64)\n",
      "(?, 768, 1, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-18-23:33:58\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpj7CjqX/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-18-23:33:59\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0174419, global_step = 200, loss = 4.75995\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.2\n",
      "dense units: 1024\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 1\n",
      "pool1 pool2 stride: 1\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7599497, 'global_step': 200, 'accuracy': 0.017441861}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "dense_units = [1024]#, 512, 256]\n",
    "batch_sizes = [16]#,32,64,256,512]\n",
    "dropouts = [0.4,0.2]\n",
    "kernel_width = [3]#,5,7]\n",
    "pool_width = [1]#,2,4]\n",
    "pool_stride = [1]#,2,4]\n",
    "\n",
    "\n",
    "#reset config, defined in previous block\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "results = []\n",
    "for dense_unit,batch_size,dropout,kernel_width,pool_width,pool_stride in product(dense_units,batch_sizes,dropouts,kernel_width,pool_width,pool_stride):\n",
    "    config['dropout'] = dropout\n",
    "    config['dense_units'] = dense_unit\n",
    "    config['kernel1_width'] = kernel_width\n",
    "    config['kernel2_width'] = kernel_width\n",
    "    config['pool1_width'] = pool_width\n",
    "    config['pool2_width'] = pool_width\n",
    "    config['pool1_stride'] = pool_stride\n",
    "    config['pool2_stride'] = pool_stride\n",
    "    \n",
    "    # Create the Estimator\n",
    "    cnn_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=None, params=config)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "    cnn_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=200, # 60000\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": dev_data},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    eval_results = cnn_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print ('\\n----setting----')\n",
    "    print ('batch_size:', batch_size)\n",
    "    print ('dropout:', dropout)\n",
    "    print ('dense units:', dense_unit)\n",
    "    print ('kernel1 kernel2 width:', kernel_width)\n",
    "    print ('pool1 pool2 width:', pool_width)\n",
    "    print ('pool1 pool2 stride:', pool_stride)\n",
    "    print ('----performance----\\n')\n",
    "    print(eval_results)\n",
    "    results.append((eval_results['accuracy'], batch_size, dropout, dense_unit, kernel_width, pool_width, pool_stride))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.017441861, 16, 0.2, 1024, 3, 1, 1), (0.011627907, 16, 0.4, 1024, 3, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpAwwZ5a\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1192a0450>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpAwwZ5a', '_save_summary_steps': 100}\n",
      "(16, 768, 1, 1)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 32)\n",
      "(16, 768, 1, 64)\n",
      "(16, 768, 1, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpAwwZ5a/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00808388  0.00824161  0.00833099 ...,  0.008348    0.00811308\n",
      "   0.00796357]\n",
      " [ 0.00837347  0.0086497   0.00868049 ...,  0.00781302  0.00818774\n",
      "   0.00795016]\n",
      " [ 0.00828907  0.0079976   0.00824392 ...,  0.00810588  0.00808703\n",
      "   0.0079982 ]\n",
      " ..., \n",
      " [ 0.00818064  0.00835777  0.00840453 ...,  0.00831238  0.00826083\n",
      "   0.00828656]\n",
      " [ 0.00811199  0.00837064  0.00854501 ...,  0.00834624  0.00809894\n",
      "   0.00827241]\n",
      " [ 0.00784457  0.00831586  0.00835719 ...,  0.00819828  0.00822433\n",
      "   0.00821106]]\n",
      "INFO:tensorflow:loss = 4.79304, step = 1\n",
      "INFO:tensorflow:global_step/sec: 3.38655\n",
      "INFO:tensorflow:loss = 4.78604, step = 101 (29.527 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpAwwZ5a/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.20663.\n",
      "(?, 768, 1, 1)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 32)\n",
      "(?, 768, 1, 64)\n",
      "(?, 768, 1, 64)\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpAwwZ5a/model.ckpt-200\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         2\n",
      "          1       0.00      0.00      0.00         2\n",
      "          2       0.00      0.00      0.00         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         2\n",
      "          5       0.00      0.00      0.00         2\n",
      "          6       0.00      0.00      0.00         2\n",
      "          7       0.00      0.00      0.00         2\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       0.00      0.00      0.00         2\n",
      "         11       0.00      0.00      0.00         2\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00         2\n",
      "         14       0.00      0.00      0.00         2\n",
      "         15       0.00      0.00      0.00         2\n",
      "         16       0.00      0.00      0.00         2\n",
      "         17       0.00      0.00      0.00         2\n",
      "         18       0.00      0.00      0.00         2\n",
      "         19       0.00      0.00      0.00         2\n",
      "         20       0.00      0.00      0.00         2\n",
      "         21       0.00      0.00      0.00         2\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       0.00      0.00      0.00         2\n",
      "         24       0.00      0.00      0.00         2\n",
      "         25       0.00      0.00      0.00         2\n",
      "         26       0.00      0.00      0.00         2\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       0.00      0.00      0.00         2\n",
      "         29       0.00      0.00      0.00         2\n",
      "         30       0.00      0.00      0.00         2\n",
      "         31       0.00      0.00      0.00         2\n",
      "         33       0.00      0.00      0.00         2\n",
      "         34       0.00      0.00      0.00         2\n",
      "         35       0.00      0.00      0.00         2\n",
      "         36       0.00      0.00      0.00         1\n",
      "         37       0.00      0.00      0.00         2\n",
      "         38       0.00      0.00      0.00         1\n",
      "         39       0.00      0.00      0.00         2\n",
      "         40       0.00      0.00      0.00         2\n",
      "         41       0.00      0.00      0.00         2\n",
      "         42       0.00      0.00      0.00         2\n",
      "         43       0.00      0.00      0.00         1\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       0.00      0.00      0.00         2\n",
      "         46       0.00      0.00      0.00         1\n",
      "         47       0.00      0.00      0.00         2\n",
      "         48       0.00      0.00      0.00         2\n",
      "         49       0.01      1.00      0.02         2\n",
      "         50       0.00      0.00      0.00         2\n",
      "         51       0.00      0.00      0.00         2\n",
      "         52       0.00      0.00      0.00         2\n",
      "         53       0.00      0.00      0.00         2\n",
      "         54       0.00      0.00      0.00         2\n",
      "         55       0.00      0.00      0.00         1\n",
      "         56       0.00      0.00      0.00         2\n",
      "         57       0.00      0.00      0.00         2\n",
      "         58       0.00      0.00      0.00         2\n",
      "         59       0.00      0.00      0.00         2\n",
      "         60       0.00      0.00      0.00         2\n",
      "         61       0.00      0.00      0.00         2\n",
      "         62       0.00      0.00      0.00         1\n",
      "         63       0.00      0.00      0.00         2\n",
      "         64       0.00      0.00      0.00         2\n",
      "         65       0.00      0.00      0.00         2\n",
      "         66       0.00      0.00      0.00         2\n",
      "         67       0.00      0.00      0.00         1\n",
      "         68       0.00      0.00      0.00         1\n",
      "         69       0.00      0.00      0.00         1\n",
      "         70       0.00      0.00      0.00         2\n",
      "         72       0.00      0.00      0.00         2\n",
      "         73       0.00      0.00      0.00         1\n",
      "         75       0.00      0.00      0.00         1\n",
      "         76       0.00      0.00      0.00         2\n",
      "         77       0.00      0.00      0.00         1\n",
      "         78       0.00      0.00      0.00         2\n",
      "         80       0.00      0.00      0.00         2\n",
      "         81       0.00      0.00      0.00         2\n",
      "         82       0.00      0.00      0.00         2\n",
      "         83       0.00      0.00      0.00         2\n",
      "         84       0.00      0.00      0.00         1\n",
      "         85       0.00      0.00      0.00         2\n",
      "         86       0.00      0.00      0.00         1\n",
      "         87       0.00      0.00      0.00         1\n",
      "         88       0.00      0.00      0.00         1\n",
      "         89       0.00      0.00      0.00         1\n",
      "         90       0.00      0.00      0.00         1\n",
      "         91       0.00      0.00      0.00         2\n",
      "         93       0.00      0.00      0.00         2\n",
      "         95       0.00      0.00      0.00         1\n",
      "         98       0.00      0.00      0.00         2\n",
      "         99       0.00      0.00      0.00         2\n",
      "        102       0.00      0.00      0.00         2\n",
      "        103       0.00      0.00      0.00         2\n",
      "        104       0.00      0.00      0.00         2\n",
      "        105       0.00      0.00      0.00         2\n",
      "        106       0.00      0.00      0.00         2\n",
      "        115       0.00      0.00      0.00         2\n",
      "        118       0.00      0.00      0.00         2\n",
      "        119       0.00      0.00      0.00         1\n",
      "        120       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.00      0.01      0.00       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "best_param = results[0]\n",
    "batch_size = best_param[1]\n",
    "config['dropout'] = best_param[2]\n",
    "config['dense_units'] = best_param[3]\n",
    "config['kernel1_width'] = best_param[4]\n",
    "config['kernel2_width'] = best_param[4]\n",
    "config['pool1_width'] = best_param[5]\n",
    "config['pool2_width'] = best_param[5]\n",
    "config['pool1_stride'] = best_param[6]\n",
    "config['pool2_stride'] = best_param[6]\n",
    "\n",
    "# Create the Estimator\n",
    "cnn_classifier = tf.estimator.Estimator(\n",
    "model_fn=cnn_model_fn, model_dir=None, params=config)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": train_full_data},\n",
    "  y=train_full_labels,\n",
    "  batch_size=batch_size,\n",
    "  num_epochs=None,\n",
    "  shuffle=True)\n",
    "\n",
    "cnn_classifier.train(\n",
    "  input_fn=train_input_fn,\n",
    "  steps=200, # 60000\n",
    "  hooks=[logging_hook])\n",
    "\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "predictions = cnn_classifier.predict(input_fn=pred_input_fn)\n",
    "predictions = list(p[\"classes\"] for p in predictions)\n",
    "print (classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
