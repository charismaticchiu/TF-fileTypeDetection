{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # This is Notebook for File Type detection using convolutional neural network on TREC-DD dataset\n",
    " # The data used is in 3 part of 256 byte frequencies format, first 256, middle, last 256\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from utility import *\n",
    "from sklearn.metrics import classification_report\n",
    "fileshape = 0\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  \n",
    "  config = params\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  \n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 256, 3, 1])\n",
    "  print(input_layer.shape)\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 1]\n",
    "  # Output Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # kernel_size specifies [width, height]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[config['kernel1_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv1.shape)\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 256, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # pool_size, strides [width, height]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[config['pool1_width'], 1], strides=[config['pool1_stride'],1])\n",
    "  print(pool1.shape)\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x1 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 32]\n",
    "  # Output Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[config['kernel2_width'], 1],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  print(conv2.shape)\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 128, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[config['pool2_width'], 1], strides=[config['pool2_stride'],1])\n",
    "  print(pool2.shape)\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 64, 1, 64]\n",
    "  # Output Tensor Shape: [batch_size, 64 * 1* 64]]\n",
    "  pool2_shape = pool2.shape\n",
    "  pool2_flat = tf.reshape(pool2, [-1, pool2_shape[1] * pool2_shape[2] * pool2_shape[3]])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 64 * 1* 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=config['dense_units'], activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 93]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=config['nclasses'])\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=config['nclasses'])\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unused_argv = ['pilot_1d_cnn_newAPI.py', 'temp_data.csv', 'model/', 1024, 512, 256]\n",
    "filename = unused_argv[1]\n",
    "\n",
    "if os.path.isfile('ft_to_idx.npy') and os.path.isfile('nclasses.npy') and os.path.isfile('group_data.npy'):\n",
    "    ft_to_idx = np.load('ft_to_idx.npy')\n",
    "    ft_to_idx = ft_to_idx.item()\n",
    "    nclasses = np.load('nclasses.npy')\n",
    "    #f = open('group_data.pkl','r')\n",
    "    #group_data = pickle.load('group_data.pkl')\n",
    "    #group_data = np.load('group_data.npy') \n",
    "    group_data = np.load('toy_data.npy') # for proof of algo purpose, real use case should use the above line\n",
    "    group_data = group_data.item()\n",
    "\n",
    "else:\n",
    "    ft_to_idx, nclasses, group_data = prepare_file(filename)\n",
    "    np.save(\"ft_to_idx\", ft_to_idx)\n",
    "    np.save(\"nclasses\", nclasses)\n",
    "    #f = open('group_data.pkl','w')\n",
    "    #pickle.dump(group_data, f)\n",
    "    np.save(\"group_data\", group_data)\n",
    "\n",
    "#gen train set\n",
    "train_full, test = train_dev_split(group_data, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train, dev = train_dev_split(train_full, proportion = 0.8, thre=1) # should be 1000 or so\n",
    "train_full_data, train_full_labels = gen_feed(train_full, ft_to_idx, upper_limit=5000)\n",
    "train_data, train_labels = gen_feed(train, ft_to_idx, upper_limit=5000)\n",
    "dev_data, dev_labels = gen_feed(dev, ft_to_idx, upper_limit=5000)\n",
    "test_data, test_labels = gen_feed(test, ft_to_idx, upper_limit=5000)\n",
    "train_full_data = train_full_data.astype(np.float32)\n",
    "train_data = train_data.astype(np.float32) \n",
    "dev_data = dev_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32) \n",
    "'''\n",
    "train_data, train_labels = np.zeros((1, group_data['application/pdf'].shape[1]-1)), np.zeros((1,))\n",
    "for i in range(100):\n",
    "tmp_data, tmp_labels = subsampled_batch(ft_to_idx, group_data, class_size=100)\n",
    "train_data = np.vstack((train_data, tmp_data))\n",
    "train_labels = np.hstack((train_labels, tmp_labels))\n",
    "train_data = np.delete(train_data,0,0)\n",
    "train_labels = np.delete(train_labels,0,0)\n",
    "train_data = train_data.astype(np.float32)\n",
    "'''\n",
    "#gen dev set\n",
    "\n",
    "'''eval_data, eval_labels = subsampled_batch(ft_to_idx, group_data, class_size=100)\n",
    "eval_data = eval_data.astype(np.float32) \n",
    "'''\n",
    "# set config\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "config['model_dir'] = unused_argv[2]\n",
    "config['n_hidden1'] = int(unused_argv[3])\n",
    "config['n_hidden2'] = int(unused_argv[4])\n",
    "config['n_hidden3'] = int(unused_argv[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpXJbSAN\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x119287950>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpXJbSAN', '_save_summary_steps': 100}\n",
      "(16, 256, 3, 1)\n",
      "(16, 256, 3, 32)\n",
      "(16, 128, 3, 32)\n",
      "(16, 128, 3, 64)\n",
      "(16, 64, 3, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpXJbSAN/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00836452  0.00843479  0.00775803 ...,  0.00818591  0.00794396\n",
      "   0.00911324]\n",
      " [ 0.00899273  0.00858209  0.00874192 ...,  0.00863636  0.00871397\n",
      "   0.00888847]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.007888    0.00798894  0.0080406  ...,  0.00808706  0.00844118\n",
      "   0.00937677]\n",
      " [ 0.0084495   0.00875259  0.00792751 ...,  0.00832587  0.00858012\n",
      "   0.00854195]\n",
      " [ 0.00815172  0.00821472  0.00834328 ...,  0.00819459  0.00809588\n",
      "   0.00835626]]\n",
      "INFO:tensorflow:loss = 19.9802, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.6297\n",
      "INFO:tensorflow:loss = 4.79922, step = 101 (9.407 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpXJbSAN/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.77624.\n",
      "(?, 256, 3, 1)\n",
      "(?, 256, 3, 32)\n",
      "(?, 128, 3, 32)\n",
      "(?, 128, 3, 64)\n",
      "(?, 64, 3, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-00:07:51\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpXJbSAN/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-00:07:52\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0116279, global_step = 200, loss = 4.75766\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.4\n",
      "dense units: 1024\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 2\n",
      "pool1 pool2 stride: 2\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7576556, 'global_step': 200, 'accuracy': 0.011627907}\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpd_fzjI\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11bfe7190>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpd_fzjI', '_save_summary_steps': 100}\n",
      "(16, 256, 3, 1)\n",
      "(16, 256, 3, 32)\n",
      "(16, 128, 3, 32)\n",
      "(16, 128, 3, 64)\n",
      "(16, 64, 3, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpd_fzjI/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00800536  0.00825992  0.00832048 ...,  0.00810248  0.00843219\n",
      "   0.00826943]\n",
      " [ 0.00806249  0.00830522  0.00894918 ...,  0.00819037  0.00829266\n",
      "   0.00797843]\n",
      " [ 0.0085142   0.00841808  0.0083434  ...,  0.00819553  0.00836972\n",
      "   0.00823922]\n",
      " ..., \n",
      " [ 0.00720072  0.00892739  0.00930677 ...,  0.0087971   0.00899999\n",
      "   0.00830541]\n",
      " [ 0.00845365  0.00857996  0.00828716 ...,  0.00804195  0.00834018\n",
      "   0.00813778]\n",
      " [ 0.00880035  0.00914334  0.00892757 ...,  0.00754298  0.00829396\n",
      "   0.00750147]]\n",
      "INFO:tensorflow:loss = 4.81983, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.4393\n",
      "INFO:tensorflow:loss = 4.77108, step = 101 (9.577 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpd_fzjI/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.78512.\n",
      "(?, 256, 3, 1)\n",
      "(?, 256, 3, 32)\n",
      "(?, 128, 3, 32)\n",
      "(?, 128, 3, 64)\n",
      "(?, 64, 3, 64)\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-00:08:16\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmpd_fzjI/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-00:08:17\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0116279, global_step = 200, loss = 4.75264\n",
      "\n",
      "----setting----\n",
      "batch_size: 16\n",
      "dropout: 0.2\n",
      "dense units: 1024\n",
      "kernel1 kernel2 width: 3\n",
      "pool1 pool2 width: 2\n",
      "pool1 pool2 stride: 2\n",
      "----performance----\n",
      "\n",
      "{'loss': 4.7526383, 'global_step': 200, 'accuracy': 0.011627907}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "dense_units = [1024]#, 512, 256]\n",
    "batch_sizes = [16]#,32,64,256,512]\n",
    "dropouts = [0.4,0.2]\n",
    "kernel_width = [3]#,5,7]\n",
    "pool_width = [2]#,4]\n",
    "pool_stride = [2]#,4]\n",
    "\n",
    "\n",
    "#reset config, defined in previous block\n",
    "config = {}\n",
    "config['nclasses'] = int(nclasses)\n",
    "results = []\n",
    "for dense_unit,batch_size,dropout,kernel_width,pool_width,pool_stride in product(dense_units,batch_sizes,dropouts,kernel_width,pool_width,pool_stride):\n",
    "    config['dropout'] = dropout\n",
    "    config['dense_units'] = dense_unit\n",
    "    config['kernel1_width'] = kernel_width\n",
    "    config['kernel2_width'] = kernel_width\n",
    "    config['pool1_width'] = pool_width\n",
    "    config['pool2_width'] = pool_width\n",
    "    config['pool1_stride'] = pool_stride\n",
    "    config['pool2_stride'] = pool_stride\n",
    "    \n",
    "    # Create the Estimator\n",
    "    cnn_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=None, params=config)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "    cnn_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=200, # 60000\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": dev_data},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    eval_results = cnn_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print ('\\n----setting----')\n",
    "    print ('batch_size:', batch_size)\n",
    "    print ('dropout:', dropout)\n",
    "    print ('dense units:', dense_unit)\n",
    "    print ('kernel1 kernel2 width:', kernel_width)\n",
    "    print ('pool1 pool2 width:', pool_width)\n",
    "    print ('pool1 pool2 stride:', pool_stride)\n",
    "    print ('----performance----\\n')\n",
    "    print(eval_results)\n",
    "    results.append((eval_results['accuracy'], batch_size, dropout, dense_unit, kernel_width, pool_width, pool_stride))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.011627907, 16, 0.4, 1024, 3, 2, 2), (0.011627907, 16, 0.2, 1024, 3, 2, 2)]\n"
     ]
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp6kndQn\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11928f790>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp6kndQn', '_save_summary_steps': 100}\n",
      "(16, 256, 3, 1)\n",
      "(16, 256, 3, 32)\n",
      "(16, 128, 3, 32)\n",
      "(16, 128, 3, 64)\n",
      "(16, 64, 3, 64)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp6kndQn/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[ 0.00822461  0.00874965  0.00777222 ...,  0.0083669   0.00876451\n",
      "   0.00848145]\n",
      " [ 0.00839658  0.00876951  0.00810262 ...,  0.00775859  0.00857296\n",
      "   0.00981512]\n",
      " [ 0.00794452  0.00734665  0.00894986 ...,  0.00826543  0.00881902\n",
      "   0.00846633]\n",
      " ..., \n",
      " [ 0.00855338  0.00833409  0.00799419 ...,  0.00833698  0.00839834\n",
      "   0.00869554]\n",
      " [ 0.00827951  0.01001852  0.00775356 ...,  0.00829127  0.0088356\n",
      "   0.0100216 ]\n",
      " [ 0.00855379  0.00900621  0.00785377 ...,  0.00807275  0.00849997\n",
      "   0.00907311]]\n",
      "INFO:tensorflow:loss = 4.78214, step = 1\n",
      "INFO:tensorflow:global_step/sec: 12.095\n",
      "INFO:tensorflow:loss = 4.48306, step = 101 (8.267 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp6kndQn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.80501.\n",
      "(?, 256, 3, 1)\n",
      "(?, 256, 3, 32)\n",
      "(?, 128, 3, 32)\n",
      "(?, 128, 3, 64)\n",
      "(?, 64, 3, 64)\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/30/zm33lx8x6673tx7yywr96tr80000gn/T/tmp6kndQn/model.ckpt-200\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         2\n",
      "          1       0.00      0.00      0.00         2\n",
      "          2       0.00      0.00      0.00         2\n",
      "          3       0.00      0.00      0.00         2\n",
      "          4       0.00      0.00      0.00         2\n",
      "          5       0.00      0.00      0.00         2\n",
      "          6       0.00      0.00      0.00         2\n",
      "          7       0.00      0.00      0.00         2\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       0.00      0.00      0.00         2\n",
      "         11       0.00      0.00      0.00         2\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.29      1.00      0.44         2\n",
      "         14       0.00      0.00      0.00         2\n",
      "         15       0.00      0.00      0.00         2\n",
      "         16       0.00      0.00      0.00         2\n",
      "         17       0.00      0.00      0.00         2\n",
      "         18       0.00      0.00      0.00         2\n",
      "         19       0.00      0.00      0.00         2\n",
      "         20       0.00      0.00      0.00         2\n",
      "         21       0.00      0.00      0.00         2\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       0.00      0.00      0.00         2\n",
      "         24       0.00      0.00      0.00         2\n",
      "         25       0.00      0.00      0.00         2\n",
      "         26       0.00      0.00      0.00         2\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       0.00      0.00      0.00         2\n",
      "         29       0.00      0.00      0.00         2\n",
      "         30       0.00      0.00      0.00         2\n",
      "         31       0.00      0.00      0.00         2\n",
      "         33       0.00      0.00      0.00         2\n",
      "         34       0.00      0.00      0.00         2\n",
      "         35       0.00      0.00      0.00         2\n",
      "         36       0.00      0.00      0.00         1\n",
      "         37       0.00      0.00      0.00         2\n",
      "         38       0.00      0.00      0.00         1\n",
      "         39       0.00      0.00      0.00         2\n",
      "         40       0.00      0.00      0.00         2\n",
      "         41       0.00      0.00      0.00         2\n",
      "         42       0.00      0.00      0.00         2\n",
      "         43       0.00      0.00      0.00         1\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       0.00      0.00      0.00         2\n",
      "         46       0.00      0.00      0.00         1\n",
      "         47       0.00      0.00      0.00         2\n",
      "         48       0.00      0.00      0.00         2\n",
      "         49       0.01      1.00      0.03         2\n",
      "         50       0.00      0.00      0.00         2\n",
      "         51       0.00      0.00      0.00         2\n",
      "         52       0.00      0.00      0.00         2\n",
      "         53       0.00      0.00      0.00         2\n",
      "         54       0.00      0.00      0.00         2\n",
      "         55       0.00      0.00      0.00         1\n",
      "         56       0.00      0.00      0.00         2\n",
      "         57       0.00      0.00      0.00         2\n",
      "         58       0.00      0.00      0.00         2\n",
      "         59       0.00      0.00      0.00         2\n",
      "         60       0.00      0.00      0.00         2\n",
      "         61       0.00      0.00      0.00         2\n",
      "         62       0.00      0.00      0.00         1\n",
      "         63       0.00      0.00      0.00         2\n",
      "         64       0.00      0.00      0.00         2\n",
      "         65       0.00      0.00      0.00         2\n",
      "         66       0.00      0.00      0.00         2\n",
      "         67       0.00      0.00      0.00         1\n",
      "         68       0.00      0.00      0.00         1\n",
      "         69       0.00      0.00      0.00         1\n",
      "         70       0.00      0.00      0.00         2\n",
      "         72       0.00      0.00      0.00         2\n",
      "         73       0.00      0.00      0.00         1\n",
      "         75       0.00      0.00      0.00         1\n",
      "         76       0.00      0.00      0.00         2\n",
      "         77       0.00      0.00      0.00         1\n",
      "         78       0.00      0.00      0.00         2\n",
      "         80       0.00      0.00      0.00         2\n",
      "         81       0.00      0.00      0.00         2\n",
      "         82       0.00      0.00      0.00         2\n",
      "         83       0.00      0.00      0.00         2\n",
      "         84       0.00      0.00      0.00         1\n",
      "         85       0.00      0.00      0.00         2\n",
      "         86       0.00      0.00      0.00         1\n",
      "         87       0.00      0.00      0.00         1\n",
      "         88       0.00      0.00      0.00         1\n",
      "         89       0.00      0.00      0.00         1\n",
      "         90       0.00      0.00      0.00         1\n",
      "         91       0.00      0.00      0.00         2\n",
      "         93       0.00      0.00      0.00         2\n",
      "         95       0.00      0.00      0.00         1\n",
      "         98       0.00      0.00      0.00         2\n",
      "         99       0.00      0.00      0.00         2\n",
      "        102       0.00      0.00      0.00         2\n",
      "        103       0.00      0.00      0.00         2\n",
      "        104       0.00      0.00      0.00         2\n",
      "        105       0.00      0.00      0.00         2\n",
      "        106       0.00      0.00      0.00         2\n",
      "        115       0.00      0.00      0.00         2\n",
      "        118       0.00      0.00      0.00         2\n",
      "        119       0.00      0.00      0.00         1\n",
      "        120       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.00      0.02      0.01       180\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "best_param = results[0]\n",
    "batch_size = best_param[1]\n",
    "config['dropout'] = best_param[2]\n",
    "config['dense_units'] = best_param[3]\n",
    "config['kernel1_width'] = best_param[4]\n",
    "config['kernel2_width'] = best_param[4]\n",
    "config['pool1_width'] = best_param[5]\n",
    "config['pool2_width'] = best_param[5]\n",
    "config['pool1_stride'] = best_param[6]\n",
    "config['pool2_stride'] = best_param[6]\n",
    "\n",
    "# Create the Estimator\n",
    "cnn_classifier = tf.estimator.Estimator(\n",
    "model_fn=cnn_model_fn, model_dir=None, params=config)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000) \n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": train_full_data},\n",
    "  y=train_full_labels,\n",
    "  batch_size=batch_size,\n",
    "  num_epochs=None,\n",
    "  shuffle=True)\n",
    "\n",
    "cnn_classifier.train(\n",
    "  input_fn=train_input_fn,\n",
    "  steps=200, # 60000\n",
    "  hooks=[logging_hook])\n",
    "\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "predictions = cnn_classifier.predict(input_fn=pred_input_fn)\n",
    "predictions = list(p[\"classes\"] for p in predictions)\n",
    "print (classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
