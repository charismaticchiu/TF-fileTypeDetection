{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "#import pickle\n",
    "from utility import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def mlp_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for MLP.\"\"\"\n",
    "  \n",
    "  #TODO: need works on this\n",
    "  config = params\n",
    "\n",
    "\n",
    "  # Input Layer\n",
    "  \n",
    "  input_layer = tf.reshape( features[\"x\"], [-1, features[\"x\"].shape[1] ] )\n",
    "  #print ('feature x', features[\"x\"])\n",
    "  #print ('feature x shape', features[\"x\"].shape)\n",
    "  #print ('reshape:', input_layer)\n",
    "  #print ('reshape shape:', input_layer.shape)\n",
    "  #trans = tf.string_to_number(input_layer)\n",
    "  #print ('trans reshape:', trans)\n",
    "  #print ('reshape shape:', input_layer.shape)\n",
    "\n",
    "\n",
    "  # Dense Layers\n",
    "  hidden1 = tf.layers.dense(inputs=input_layer, units=config['n_hidden1'], activation=tf.nn.relu)\n",
    "  normal_1 = tf.layers.batch_normalization(hidden1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    " \n",
    "  drop_h1 = tf.layers.dropout(\n",
    "      inputs=normal_1, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  hidden2 = tf.layers.dense(inputs=drop_h1, units=config['n_hidden2'], activation=tf.nn.relu)\n",
    "  drop_h2 = tf.layers.dropout(\n",
    "      inputs=hidden2, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  if config['n_hidden3'] != None:\n",
    "    hidden3 = tf.layers.dense(inputs=drop_h2, units=config['n_hidden3'], activation=tf.nn.relu)\n",
    "    drop_h3 = tf.layers.dropout(\n",
    "        inputs=hidden3, rate=config['dropout'], training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=drop_h3, units=config['nclasses'])\n",
    "  else:\n",
    "    logits = tf.layers.dense(inputs=drop_h2, units=config['nclasses'])\n",
    "    \n",
    "  predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  # obsolete logits = tf.Print(logits,predictions[\"probabilities\"])\n",
    "  \n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=config['nclasses'])\n",
    "  #print (onehot_labels)\n",
    "  #print (logits)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.001)\n",
    "\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  print ('prob shape',predictions[\"probabilities\"].shape)\n",
    "  print ('prob ',predictions[\"probabilities\"])\n",
    "  print ('prob ',predictions[\"probabilities\"])\n",
    "  k_3 = tf.nn.in_top_k(predictions=predictions[\"probabilities\"], targets= labels,k = 3)\n",
    "  k_3 = tf.cast(k_3, tf.float32)\n",
    "  print (k_3)\n",
    "  eval_metric_ops = {\n",
    "      #\"accuracy\": tf.metrics.accuracy(\n",
    "      #    labels=labels, predictions=predictions[\"classes\"]),\n",
    "      \"top-3 accuracy\": tf.metrics.average_precision_at_k(\n",
    "                        labels = labels,\n",
    "                        predictions = predictions[\"probabilities\"],\n",
    "                        k = 3)\n",
    "          \n",
    "      #\"top-5 accuracy\": tf.metrics.mean(\n",
    "      #   tf.nn.in_top_k(predictions=predictions[\"probabilities\"], \n",
    "      #                   targets= labels,k = 5)) \n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unused_argv = ['pilot_mlp_newAPI.py', 'data_1p_companded.csv', 'model/', 1024, 512, 256]\n",
    "\n",
    "filename = unused_argv[1]\n",
    "\n",
    "'''\n",
    "if os.path.isfile('ft_to_idx.npy') and os.path.isfile('idx_to_ft.npy') and os.path.isfile('nclasses.npy') and os.path.isfile('group_data.npy'):\n",
    "    ft_to_idx = np.load('ft_to_idx.npy')\n",
    "    ft_to_idx = ft_to_idx.item()\n",
    "    idx_to_ft = np.load('idx_to_ft.npy')\n",
    "    idx_to_ft = idx_to_ft.item()\n",
    "    nclasses = np.load('nclasses.npy')\n",
    "    #f = open('group_data.pkl','r')\n",
    "    #group_data = pickle.load('group_data.pkl')\n",
    "    group_data = np.load('group_data.npy') \n",
    "    #group_data = np.load('toy_data.npy') # for proof of algo purpose, real use case should use the above line\n",
    "    group_data = group_data.item()\n",
    "\n",
    "else:\n",
    "    ft_to_idx, idx_to_ft, nclasses, group_data = prepare_file(filename)\n",
    "    np.save(\"ft_to_idx\", ft_to_idx)\n",
    "    np.save(\"idx_to_ft\", idx_to_ft)\n",
    "    np.save(\"nclasses\", nclasses)\n",
    "    #f = open('group_data.pkl','w')\n",
    "    #pickle.dump(group_data, f)\n",
    "    np.save(\"group_data\", group_data)\n",
    "\n",
    "#gen train set\n",
    "cnt = map(lambda x: len(x),group_data.values())\n",
    "percentile_40 = np.percentile(cnt, 40)\n",
    "train, dev ,test = train_dev_test_split(group_data, proportion = [0.6,0.2], thre = percentile_40) # should be 1000 or so\n",
    "train_data, train_labels = gen_feed(train, ft_to_idx, upper_limit=12000)\n",
    "del train, group_data\n",
    "dev_data, dev_labels = gen_feed(dev, ft_to_idx, upper_limit=3000)\n",
    "del dev\n",
    "test_data, test_labels = gen_feed(test, ft_to_idx, upper_limit=3000)\n",
    "del test\n",
    "train_data = train_data.astype(np.float32) \n",
    "dev_data = dev_data.astype(np.float32)\n",
    "test_data = test_data.astype(np.float32) \n",
    "'''\n",
    "ft_to_idx = np.load('ft_to_idx.npy')\n",
    "ft_to_idx = ft_to_idx.item()\n",
    "idx_to_ft = np.load('idx_to_ft.npy')\n",
    "idx_to_ft = idx_to_ft.item()\n",
    "\n",
    "dev_data = np.load(\"dev_data.npy\")\n",
    "dev_labels = np.load(\"dev_labels.npy\")\n",
    "test_data = np.load(\"test_data.npy\")\n",
    "test_labels = np.load(\"test_labels.npy\")\n",
    "train_data = np.load(\"train_data.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data_norm = scaler.fit_transform(train_data)\n",
    "dev_data_norm = scaler.transform(dev_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "hidden_units = [[1024,1024],[1024,512,256],[256,128,100]]#,[2048,2048],[2048,1024,512]]\n",
    "batch_sizes = [64,128]#,256,512]\n",
    "dropouts = [0.4,0.2]\n",
    "\n",
    "#reset config, defined in previous block\n",
    "config = {}\n",
    "config['nclasses'] = len(np.unique(train_labels))\n",
    "results = []\n",
    "for hidden, batch_size, dropout in product(hidden_units,batch_sizes,dropouts):\n",
    "    config['n_hidden1'] = hidden[0]\n",
    "    config['n_hidden2'] = hidden[1]\n",
    "    try: config['n_hidden3'] = hidden[2]\n",
    "    except: config['n_hidden3'] = None\n",
    "    config['dropout'] = dropout\n",
    "    # Create the Estimator\n",
    "    mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn, model_dir=None, params=config)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=5000)\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=3,\n",
    "      shuffle=True)\n",
    "\n",
    "    steps = train_data.shape[0]/batch_size\n",
    "    #print (steps)\n",
    "    mlp_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=None, # 60000\n",
    "      hooks=[logging_hook])\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": dev_data},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "    eval_results = mlp_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print ('----setting----')\n",
    "    print ('hidden units:',hidden)\n",
    "    print ('batch_size:', batch_size)\n",
    "    print ('dropout:', dropout)\n",
    "    print ('----performance----')\n",
    "    print(eval_results)\n",
    "    results.append((eval_results['accuracy'], hidden, batch_size, dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_param = results[0]\n",
    "hidden = best_param[1]\n",
    "config['n_hidden1'] = hidden[0]\n",
    "config['n_hidden2'] = hidden[1]\n",
    "try: config['n_hidden3'] = hidden[2]\n",
    "except: config['n_hidden3'] = None\n",
    "batch_size = best_param[2]\n",
    "config['dropout'] = best_param[3]\n",
    "\n",
    "# Create the Estimator\n",
    "cnn_classifier = tf.estimator.Estimator(\n",
    "model_fn=mlp_model_fn, model_dir=None, params=config)\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=5000) \n",
    "\n",
    "#make full training data\n",
    "train_full_data = np.vstack((train_data,dev_data))\n",
    "train_full_labels = np.hstack((train_labels,dev_labels))\n",
    "train_full_data = scaler.fit_transform(train_full_data)\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": train_full_data},\n",
    "  y=train_full_labels,\n",
    "  batch_size=batch_size,\n",
    "  num_epochs=3,\n",
    "  shuffle=True)\n",
    "\n",
    "steps = train_full_data.shape[0]/batch_size\n",
    "cnn_classifier.train(\n",
    "  input_fn=train_input_fn,\n",
    "  steps=None, # 60000\n",
    "  hooks=[logging_hook])\n",
    "\n",
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": test_data},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "predictions = mlp_classifier.predict(input_fn=pred_input_fn)\n",
    "print (list(p[\"probabilities\"] for p in predictions))\n",
    "predictions = list(p[\"classes\"] for p in predictions)\n",
    "\n",
    "print ('----Best setting----')\n",
    "print ('hidden units:',hidden)\n",
    "print ('batch_size:', batch_size)\n",
    "print ('dropout:', dropout)\n",
    "print ('\\n')\n",
    "\n",
    "target_names = np.unique(test_labels)\n",
    "target_names = [idx_to_ft[i] for i in target_names]\n",
    "print (classification_report(test_labels, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniq = np.unique(train_full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i , lab in enumerate(uniq):\n",
    "    print (i, target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(train_labels) == np.unique(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
